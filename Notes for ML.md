## 机器学习笔记

干货：

- leetcode
- 实习经验非常重要，有实习做过相关项目的话很加分 

###  一些问题：

- boosting

  **提升方法**（**boosting**）是一种通过组合弱学习器来产生强学习器的通用且有效的方法。在分类问题中，它通过改变训练样本的权重（增加分错样本的权重，减小分队样本的的权重），学习多个分类器，并将这些分类器线性组合，提高分类器性能。boosting数学表示为： 

  $f(x)=w_0+∑_{m=1}^Mw_mϕ_m(x)$

  其中$w$是权重，$ϕ$是弱分类器的集合，可以看出最终就是基函数的线性组合。

  **AdaBoost：**让每一个新加入的弱学习器都体现出一些新的数据中的模式，为了实现这一点，AdaBoost为每一个训练样本维护一个权重分布。AdaBoost分多轮训练，在训练的第$t$轮，我们更新样本的权重到$D_t$并训练一个弱学习器，使得该学习器在权重分布![D_t](http://www.zhihu.com/equation?tex=D_t)上产生最小的加权训练误差。在第一轮中，所有样本权重相同,为 $\frac{1}{n\_samples}$ 。在之后的每一轮中,我们提高误分类样本的权重，降低准确分类样本的权重。这样我们就使得每一轮的弱学习器都更多的聚焦于上一轮中较难被准确分类的样本。

  **RankBoost**

  **XgBoost**

  **Gradient Boosting**：它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能（一般为拟合程度+正则项），认为损失函数越小，性能越好。

  **GBDT(Gradient Boosting Decision Tree)：**以决策树为基函数的提升方法称为提升树，其决策树可以是分类树OR回归树。提升树模型可以表示为决策树的加法模型。 

  以决策树为基函数的提升方法称为提升树，其决策树可以是分类树OR回归树。提升树模型可以表示为决策树的加法模型。 

  $f_M(x)=∑_{m=1}^MT(x;Θ_m)$

  其中，$T(x;Θ_m)$表示决策树，$Θ_m$表示树的参数，$M$为树的个数。

  利用了损失函数的负梯度在当前模型的值 

  $−[\frac{∂L(y,f(x_i))}{∂f(x_i)}]_{f(x)=f_{m−1}(x)}$

  作为回归问题提升树算法的残差近似值，去拟合一个回归树。

  **决策树：**

  1. ID3决策树，$Gain(D,a)=Ent(D)-\sum ^V_{v=1}\frac{|D^v|}{|D|}Ent(D^v)$ 优先用能区分度大的也就是信息增益大的属性来进行划分，泛化能力弱；
  2. C4.5决策树，$GainRatio(D,T)=\frac {Gain(D,T)}{IV(T)}$.  $IV(T)=-\sum ^V_{v=1}\frac{∣∣D^v∣∣}{|D|}\log_2\frac{∣∣D^v∣∣}{|D|}$ ,成为分支标准T的“固有值”(intrinsic value)先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的，适用大数据集；
  3. CART决策树，$Ginisplit(T)=\sum^k_{i=1}\frac{N_i}{N}gini(T_i)$, $gini(T_i)=1−\sum^k_{j=1}p_j^2$ 在候选属性中选择基尼系数最小的属性作为最优划分属性，GINI指数主要是度量数据划分的不纯度，是介于0~1之间的数。GINI值越小，表明样本集合的纯净度越高；GINI值越大表明样本集合的类别越杂乱，二叉树，可处理连续型数据；~~信息熵在x=1处一阶泰勒展开就是基尼指数~~


- L1和L2正则

  机器学习的目标，实际上就是找到一个足够好的函数用以预测真实规律，使得全局损失 $L(F)$最小。但是它只考虑了对数据的拟合，而忽视了模型本身的复杂度，可能过拟合。

  L0正则，使尽可能多的参数为零，使模型稀疏化；但是L0正则难以在多项式时间内找到有效解，L1范数是L0范数最紧的凸放松，

   L1正则项，会倾向于使得参数稀疏化；而使用 L2正则项，会倾向于使得参数稠密地接近于零；~~可以用二个参数的等值线图来说明，L1正则可用公式推导~~

- LR，SVM，KNN和random forest

  **逻辑回归(LR)：**套用逻辑函数$P(Y=1|x, w)=\frac{e^{w⋅x+b}}{1+e^{w⋅x+b}}$$l(θ)=∑y\log g(θ^Tx+b)+(1−y)\log (1−g(θ^Tx+b))$

  LR与SVM本质区别是损失函数不同；逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本；逻辑回归是统计方法，svm是几何方法；在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。

  **KNN：**近邻间的距离会被大量的不相关属性所支配。

  **random forest：**随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出

- 怎么理解Dropout

  **dropout：**深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。

- DNN为什么功能强大，说说你的理解

- 优化方法

  **SGD：**每一次迭代计算mini-batch的梯度，然后对参数进行更新。选择合适的learning rate比较困难；容易收敛到局部最优

  **Adagrad ：**$n_t=n_{t-1}+g_t^2$ , $\Delta{\theta_t}=-\frac{\eta}{\sqrt{n_t+\epsilon}}*g_t$。 前期$g_t$较小的时候， regularizer较大，能够放大梯度，后期$g_t$较大的时候，regularizer较小，能够约束梯度，适合处理稀疏梯度；中后期梯度趋近0，使得训练提前结束

  - 后期![g_t](https://www.zhihu.com/equation?tex=g_t)较大的时候，regularizer较小，能够约束梯度 
  - 适合处理稀疏梯度 

- ensemble

  训练多个模型做组合

- 介绍下Maxout

- 预处理 模型融合

- 详细说一个你知道的优化算法(Adam等)

- ​

- stacking是什么？需要注意哪些问题

- 了解哪些online learning的算法

- 如何解决样本不均衡的问题

  Sampling：数据重采样，根据已有数据生成新的数据；采用其他有效的评价指标(ROC，AUC)；修正数据分布(增加小类样本的权值)

- fasterRCNN中的ROIPooling是如何实现的

- 如何进行特征的选择

- 如何进行模型的选择

- 原问题与对偶问题

- 判别模型与生成模型

- 激活函数

  **Sigmoid：**$f(x) = \frac1{1+e^{-x}}$ 激活函数计算量大，反向传播求误差梯度时，求导涉及除法
  反向传播时，很容易就会出现梯度消失的情况

  **tanh：**$tanh(x)=2sigmoid(2x)−1$ tanh 是 0 均值的，因此实际应用中 tanh 会比 sigmoid 更好

  **ReLU：**$f(x)=max(0,x)$ SGD 的收敛速度快；训练的时候很”脆弱”，很容易就”die”了，要小心选择 learning rate

  **softmax：**$\sigma (z)_j = \frac{e^{z_j}}{\sum_{k=1}{K}e^{z_k}}$ 用于多分类

- 梯度消失

  梯度消失问题发生时，接近于输出层的hidden layer等的权值更新相对正常，但前面的hidden layer的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值。本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。另外，LSTM的结构设计也可以改善RNN中的梯度消失问题。

- 常用的有哪些损失函数

- XX用户画像挖掘怎么做的feature engineering?

- 假设一个5*5的filter与图像卷积，如何降低计算量？

- 做过模型压缩吗？介绍下

- 什么是residual learning？说说你的理解

- residual learning所说的residual和GBDT中的residual有什么区别？

- FFM和FTRL有过了解吗？

- 你对现在Deep Learning的发展和遇到的问题有什么看法？



### 自然语言处理

1、 负责词法分析、自动对话、语义挖掘和语言逻辑等相关研究工作；
2、 负责自然语言处理的算法研发，包括但不限于语义分析、意图识别、人机对话、机器翻译、知识图谱、命名实体识别等；

#### 命名实体

- 规则：词法、语法构成、上下文搭配、用词等规律,槽等形式

识别准确率高，但是规则覆盖是有限的，效率不高

- 统计：HMM , SVM , ME , CRF



#### 知识图谱

用三元组 h, r, t 的形式描述数据；head: 头部实体 / tail: 尾部实体 / relation: 关系/属性；类(category)、实体(entity)和关系(relation)

关系分为两类:
 描述实体和类之间类属关系的属性is_a,相关的三元组可写作category(entity),
例如company(Apple)
 描述实体之间关系的属性R,相关的三元组可写作R(entity1, entity2),例如
acquire(Google, MetaWeb)



知识的嵌入表示：用低维连续向量表示实体与关系

知识图谱的不平衡性：

- 引入稀疏矩阵，相比低秩矩阵更有优势
- 引入高斯分布


#### word2vec

word2vec：它的输出节点数是 V 个，对应了 V 个词语，本质是一个多分类问题

**Skip-gram：** 模型则根据**中心词W(t)**来预测周围词

**CBOW：**模型根据**中心词W(t)周围的词**来预测中心词

**one-hot encoder：**本质上是用一个只含一个 1、其他都是 0 的向量来唯一表示词语




### 参考

